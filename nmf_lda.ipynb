{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website: https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwc/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF\n",
      "Topic 0: don just people think like know time good right ve\n",
      "Topic 1: card video monitor drivers cards bus vga driver color ram\n",
      "Topic 2: god jesus bible christ faith believe christian christians church sin\n",
      "Topic 3: game team year games season players play hockey win player\n",
      "Topic 4: car new 00 sale 10 price offer condition shipping 20\n",
      "Topic 5: thanks does know advance mail hi anybody info looking help\n",
      "Topic 6: windows file use files dos window program using problem running\n",
      "Topic 7: edu soon cs university com email internet article ftp send\n",
      "Topic 8: key chip encryption clipper keys government escrow public use algorithm\n",
      "Topic 9: drive scsi drives hard disk ide controller floppy cd mac\n",
      "\n",
      "\n",
      "LDA\n",
      "Topic 0: people gun armenian armenians war turkish states israel said children\n",
      "Topic 1: government people law mr use president don think right public\n",
      "Topic 2: space program output entry data nasa use science research build\n",
      "Topic 3: key car chip used keys bike use bit clipper number\n",
      "Topic 4: edu file com available mail ftp files information image send\n",
      "Topic 5: god people does jesus say think believe don know just\n",
      "Topic 6: windows use drive thanks does problem know card like using\n",
      "Topic 7: ax max b8f g9v a86 pl 145 1d9 0t 34u\n",
      "Topic 8: just don like think know good time ve people said\n",
      "Topic 9: 10 00 25 15 12 20 11 14 17 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx),\n",
    "              \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 10\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online',\n",
    "                                learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "\n",
    "print('NMF')\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print('\\n')\n",
    "print('LDA')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bleach Report articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7290289e978b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtitles_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'article_dicts_saturday2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtitles_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "docs_raw_filename = \"docs_raw_saturday2\"\n",
    "with open (docs_raw_filename, 'rb') as fp:\n",
    "    raw_docs = pickle.load(fp)\n",
    "\n",
    "docs_cleaned_filename = \"docs_cleaned_saturday2\"\n",
    "with open (docs_cleaned_filename, 'rb') as fp:\n",
    "    cleaned_docs = pickle.load(fp)\n",
    "\n",
    "titles_filename = 'article_dicts_saturday2'\n",
    "with open (titles_filename, 'rb') as fp:\n",
    "    titles = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwc/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF\n",
      "Topic 0: points game series celtics rebounds season games rockets assists jazz\n",
      "Topic 1: nba triple mj 30 way best liangelo chanceraptors classmikal clowning\n",
      "Topic 2: draft nfl browns lb landry football app wnba journey st\n",
      "Topic 3: year season said team nba players league like knicks million\n",
      "Topic 4: james cleveland cavaliers lebron cavs thomas lue trade love nance\n",
      "Topic 5: warriors curry durant golden state kerr thompson green stephen klay\n",
      "Topic 6: betting oddsshark odds cover picks games check nba point total\n",
      "Topic 7: embiid philadelphia 76ers sixers heat miami fultz simmons joel game\n",
      "Topic 8: tournament ncaa michigan villanova kansas duke state wildcats sg sf\n",
      "Topic 9: leonard spurs antonio san popovich kawhi aldridge wright wojnarowski meeting\n",
      "\n",
      "\n",
      "LDA\n",
      "Topic 0: percent season points year game point shooting nba just ball\n",
      "Topic 1: game games nba vs conference state warriors round series rockets\n",
      "Topic 2: million free trade year season team lakers pick deal contract\n",
      "Topic 3: draft tournament state nfl ncaa teams best college overall kansas\n",
      "Topic 4: said nba like just team time players league year game\n",
      "Topic 5: james team season cleveland cavaliers coach nba time lebron cavs\n",
      "Topic 6: points game rebounds season games team assists just point win\n",
      "Topic 7: sf sg suns jr nba hawks draft phoenix pf pg\n",
      "Topic 8: nba mj best way triple 30 meek welcomed game recordcollin\n",
      "Topic 9: philadelphia embiid 76ers sixers heat miami simmons game fultz wade\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx),\n",
    "              \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "# documents = dataset.data\n",
    "documents = raw_docs\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 10\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online',\n",
    "                                learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "\n",
    "print('NMF')\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print('\\n')\n",
    "print('LDA')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
