\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{nips_2017_abstract}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Basketball Article Recommender\\
for Bleacher Report}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Stephen Albro\\
  Master of Business Analytics\\
  Massachusetts Institute of Technology\\
  Cambridge, MA 02139\\
  \texttt{salbro@mit.edu}\\
  %% examples of more authors
   \And
   Cyrille Combettes \\
   Master of Business Analytics\\
  Massachusetts Institute of Technology\\
  Cambridge, MA 02139\\
   \texttt{cyrille@mit.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

% \begin{abstract}
%   The abstract paragraph should be indented \nicefrac{1}{2}~inch
%   (3~picas) on both the left- and right-hand margins. Use 10~point
%   type, with a vertical spacing (leading) of 11~points.  The word
%   \textbf{Abstract} must be centered, bold, and in point size 12. Two
%   line spaces precede the abstract. The abstract must be limited to
%   one paragraph.
% \end{abstract}

\section{Abstract and Motivation}
% This project aims at using machine learning to generate ESPN articles.
% The idea is to write a program to scrape a bunch of articles to form a training corpus,
% and then build machine learning algorithms to generate new articles.
As avid basketball fans, we needed to keep up-to-date on the National Basketball Association during our busy MBAn year. Bleacher Report, a primary digital destination for sports readers, kept us up to speed all year with its well-written articles, and so we thought about returning the favor in a project.

In an age of cut-throat digital competition, it is vital for websites to retain visitors. Bleacher Report's hope for a visitor is two-fold. First, that the visitor jumps from article to article on the website, rather than returning to the search engine results page. Second, that the visitor falls in love with the content and coverage of Bleacher Report's articles, and develops a
loyalty to the website.

We feel that having a micro-targeted article recommendation system would provide Bleacher 
Report with an analytics-based edge over its competitors. In this project,
we developed a prototype for an article recommendation system using state of the art NLP techniques.  Using our best model, we also produced visualizations of the Bleacher Report topic landscape and discovered interesting surprises.


%Specifically,we aim to cluster over 3000 Bleacher Report articles by testing three different approaches
%for topic modeling: Word2Vec, Latent Dirichlet Allocation (LDA), 
%and Non-negative Matrix Factorization (NMF).

\section{Data collection and preprocessing}

% We will query articles from the ESPN API, available at
% \url{http://www.espn.com/static/apis/devcenter/docs/headlines.html#what-it-returns}.
% There are parameters that we can use in the query to return news for a specific date,
% to disable specific information, to access to top stories as selected by ESPN editorial staff,
% to access to news for a particular sport,
% or to access to stories about a particular athlete, for example.

We queried Bleacher Report NBA
articles from October 17, 2017, the start of the 2017-2018 NBA season. Our entire query consisted of
joining words like \emph{basketball} and \emph{NBA} with all 30 teams and the top 25 active players:
we received 3,314 URLs in return.  We then wrote a script to request the webpage for each URL and scrape its content, picking the paragraphs out of the HTML.

There were a number of preprocessing steps we had to do before our articles were ready for analysis. Because our primary goal was text-based clustering, a bag-of-words cleaning approach was preferred 
over one that preserved punctuation and grammar.

\section{Key results}
A reasonable document-recommendation strategy embed documents as vectors and then use K Nearest Neighbors to group documents together.  The first step in our project, then, was to choose an article embedding strategy.  We experimented with three primary methods: word2vec, Non-negative Matrix Factorization (NMF), and Latent Dirichlet Allocation (LDA). The later two methods discover underlying \textit{topics} in the articles and then express each document as a distribution over those topics.  We evaluated each approach using a hand-crafted test set. 

A preliminary analysis revealed LDA to be the most promising strategy, and so we focused the rest of our efforts on developing a quality LDA topic model, which we then use to produce visualizations of the topic landscape of Bleacher Report's NBA articles. To our surprise, the final clusters revealed that a couple of other major sports leagues snuck into our primarily NBA-driven corpus ...


%The idea of our recommendation algorithm is to (1) generate topics from our corpus once for all
%before (2)
%recommending articles using a nearest-neighbor approach over the topic distributions of the reader's 
%recent articles. We try both LDA and NMF, and it turns out LDA has the best results for our experiment.
%Thus, we then explore the LDA-based recommmendation algorithm more in depth and display
%insightful visualizations. Topics include \emph{Stats}, \emph{Trade rumors}, \emph{Cleveland Cavaliers},
%\emph{NCAA}, and \emph{Outliers\_NFL} and \emph{Outliers\_EuropeanSoccer}.

% \section{Impact}
% 
% The impact of this project is to evaluate if ESPN articles follow a recognizable pattern.
% It is widely known that an article on a basketball game, for example, is written during -- and even before! --
% the game, and we want to investigate to what extent the article is \emph{independant} of the specifics
% of the game: is the article a frame in which the reporter would just enter values for the score, the winning
% team, the losing team, and some lexical field words based on the reader's expected opinion? Or are the ESPN
% reporters true novelists? Answering this, we will assess if ESPN articles are a scam to put into the fake
% news category, or interesting and constructive information.

\end{document}
