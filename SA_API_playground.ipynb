{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pickle\n",
    "from utils import *\n",
    "# from newsapi import NewsApiClient # !pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"b942a2d2391e4ad8ae01c9168a5cdcc1\" # salbro@mit.edu\n",
    "API_KEY = \"90493a497ace4b8ebe8ef5e82bfadce6\" # cyrille's API key\n",
    "# API_KEY = \"f604134c17ae431591a2976d7c3bae55\" # stephenpalbro@gmail.com api key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a massive NBA query consisting of \"NBA\" + all teams + top 25 players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all NBA teams, add them to the query\n",
    "with open(\"teams.txt\", \"r\") as fp:\n",
    "    teams = [\"(\" + team.replace(\"\\n\", \"\").strip().lower() + \")\" for team in fp.readlines()]\n",
    "\n",
    "query = \"NBA OR \" + \" OR \".join(teams)\n",
    "query += \" OR \"\n",
    "\n",
    "# add all players to the query\n",
    "query += \" OR \".join([\"(\"+name+\")\" for name in TOP_100_PLAYERS[:25]]) # top 25 players\n",
    "\n",
    "# URL-ify the query\n",
    "query = urlify(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get the articles now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3272...50/3272...100/3272...150/3272...200/3272...250/3272...300/3272...350/3272...400/3272...450/3272...500/3272...550/3272...600/3272...650/3272...700/3272...750/3272...800/3272...850/3272...900/3272...950/3272...1000/3272...1050/3272...1100/3272...1150/3272...1200/3272...1250/3272...1300/3272...1350/3272...1400/3272...1450/3272...1500/3272...1550/3272...1600/3272...1650/3272...1700/3272...1750/3272...1800/3272...1850/3272...1900/3272...1950/3272...2000/3272...2050/3272...2100/3272...2150/3272...2200/3272...2250/3272...2300/3272...2350/3272...2400/3272...2450/3272...2500/3272...2550/3272...2600/3272...2650/3272...2700/3272...2750/3272...2800/3272...2850/3272...2900/3272...2950/3272...3000/3272...3050/3272...3100/3272...3150/3272...3200/3272...3250/3272...CPU times: user 2min 40s, sys: 9.85 s, total: 2min 50s\n",
      "Wall time: 34min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_dict = {\"from\": \"2017-10-17\", \"sources\":\"bleacher-report\",\n",
    "              \"pageSize\":\"100\", \"apiKey\":API_KEY, \"q\":query} # you can also do \"sortBy=popularity\"\n",
    "set_of_docs = get_docs_from_params(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_filename = \"all_nba_from_oct17\"\n",
    "with open(big_filename, 'wb') as fp:\n",
    "    pickle.dump(set_of_docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (big_filename, 'rb') as fp:\n",
    "#     docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# players only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(lebron james) OR lebron\n",
      "0/845...50/845...100/845...150/845...200/845...250/845...300/845...350/845...400/845...450/845...500/845...550/845...600/845...650/845...700/845...750/845...800/845...(kevin durrant) OR KD\n",
      "0/30...(steph curry) OR (stephen curry)\n",
      "0/351...50/351...100/351...150/351...200/351...250/351...300/351...350/351...(james harden) OR (harden)\n",
      "0/210...50/210...100/210...150/210...200/210...(russel westbrook) OR (westbrook)\n",
      "0/242...50/242...100/242...150/242...200/242...(anthony davis)\n",
      "0/201...50/201...100/201...150/201...200/201..."
     ]
    }
   ],
   "source": [
    "player_queries = [\"(lebron james) OR lebron\", \"(kevin durant) OR KD\", \"(steph curry) OR (stephen curry)\", \n",
    "                 \"(james harden) OR (harden)\", \"(russel westbrook) OR (westbrook)\", \"(anthony davis)\"]\n",
    "player_docs_list = {}\n",
    "\n",
    "start_date = \"2017-10-17\" # start of 2017-2018 NBA season\n",
    "for player, name in zip(player_queries, player_names):\n",
    "    print(name)\n",
    "    param_dict = {\"from\": start_date, \"sources\":\"bleacher-report\",\n",
    "              \"pageSize\":\"100\", \"apiKey\":API_KEY, \"q\":player} # you can also do \"sortBy=popularity\"\n",
    "\n",
    "    player_docs = get_docs_from_params(param_dict)\n",
    "    player_docs_list[player] = player_docs\n",
    "\n",
    "# save to a file\n",
    "player_names = [\"bron\", \"kd\", \"steph\", \"harden\", \"russel\", \"davis\"]\n",
    "for player, name in zip(player_queries, player_names):\n",
    "    filename = name + \"_from_oct17\"\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(player_docs_list[player], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scrape the comments ... having trouble with this because of the iframe. forget about it for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 394 ms, sys: 36.2 ms, total: 430 ms\n",
      "Wall time: 848 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# comments_by_doc = []\n",
    "# for article in all_articles[:10]:\n",
    "#     comments = []\n",
    "#     url = article['url']\n",
    "#     soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "#     content = soup.find_all(\"span\", {\"class\":\"_5mdd\"})\n",
    "#     if content is None:\n",
    "#         print(\"no comment with class _5mdd\")\n",
    "#     for comment_span in content:\n",
    "#         comments.append(comment_span.get_text())\n",
    "#     comments_by_doc.append(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
