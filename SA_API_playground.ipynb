{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pickle\n",
    "from utils import *\n",
    "# from newsapi import NewsApiClient # !pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"b942a2d2391e4ad8ae01c9168a5cdcc1\" # salbro@mit.edu\n",
    "API_KEY = \"90493a497ace4b8ebe8ef5e82bfadce6\" # cyrille's API key\n",
    "# API_KEY = \"f604134c17ae431591a2976d7c3bae55\" # stephenpalbro@gmail.com api key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a massive NBA query consisting of \"NBA\" + all teams + top 25 players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all NBA teams, add them to the query\n",
    "with open(\"data/teams.txt\", \"r\") as fp:\n",
    "    teams = [\"(\" + team.replace(\"\\n\", \"\").strip().lower() + \")\" for team in fp.readlines()]\n",
    "\n",
    "query = \"NBA OR \" + \" OR \".join(teams)\n",
    "query += \" OR \"\n",
    "\n",
    "# add all players to the query\n",
    "query += \" OR \".join([\"(\"+name+\")\" for name in TOP_100_PLAYERS[:25]]) # top 25 players\n",
    "\n",
    "# URL-ify the query\n",
    "query = urlify(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\"from\": \"2017-10-17\", \"sources\":\"bleacher-report\",\n",
    "              \"pageSize\":\"100\", \"apiKey\":API_KEY, \"q\":query} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = get_articles(param_dict)\n",
    "all_articles = []\n",
    "for response in responses:\n",
    "    all_articles += response['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3314 50/3314 100/3314 150/3314 200/3314 250/3314 300/3314 350/3314 400/3314 450/3314 500/3314 550/3314 600/3314 650/3314 700/3314 750/3314 800/3314 850/3314 900/3314 950/3314 1000/3314 1050/3314 1100/3314 1150/3314 1200/3314 1250/3314 1300/3314 1350/3314 1400/3314 1450/3314 1500/3314 1550/3314 1600/3314 1650/3314 1700/3314 1750/3314 1800/3314 1850/3314 1900/3314 1950/3314 2000/3314 2050/3314 2100/3314 2150/3314 2200/3314 2250/3314 2300/3314 2350/3314 2400/3314 2450/3314 2500/3314 2550/3314 2600/3314 2650/3314 2700/3314 2750/3314 2800/3314 2850/3314 2900/3314 2950/3314 3000/3314 3050/3314 3100/3314 3150/3314 3200/3314 3250/3314 3300/3314 "
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for i, article in enumerate(all_articles):\n",
    "    if i % 50 == 0:\n",
    "        print(str(i)+\"/\"+str(len(all_articles)), end=\" \")\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(article['url']).text, \"lxml\")\n",
    "    content = soup.find(\"div\", {\"class\":\"organism contentStream\"})\n",
    "    if content is None:\n",
    "        content_slides = soup.find_all(\"div\", {\"class\":\"organism contentStream slide\"})\n",
    "        paragraphs = []\n",
    "        for content_slide in content_slides:\n",
    "            paragraphs += content_slide.find_all(\"p\")\n",
    "    else:\n",
    "        paragraphs = content.find_all(\"p\")\n",
    "\n",
    "    entire_text = \"\".join([p.get_text() for p in paragraphs])\n",
    "    docs.append(entire_text)\n",
    "    time.sleep(.300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save RAW DOCS\n",
    "raw_docs = [d for d in docs]\n",
    "docs_raw_filename = \"docs_raw_saturday2\"\n",
    "with open(docs_raw_filename, 'wb') as fp:\n",
    "    pickle.dump(raw_docs, fp)\n",
    "    \n",
    "# with open (docs_raw_filename, 'rb') as fp:\n",
    "#     raw_docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/coaches.txt\", \"r\") as fp:\n",
    "    coaches = [team.replace(\"\\n\", \"\").strip().lower() for team in fp.readlines()]\n",
    "    \n",
    "with open(\"data/teams.txt\", \"r\") as fp:\n",
    "    teams = [team.replace(\"\\n\", \"\").strip().lower() for team in fp.readlines()]\n",
    "    \n",
    "df = pd.read_csv(\"data/cities_teams.csv\", header=None)\n",
    "\n",
    "cities = [v.lower() for v in df[0].values]\n",
    "team_names = [v.lower().strip() for v in df[1].values]\n",
    "\n",
    "players_first_names, players_last_names = zip(*[p.split() for p in TOP_100_PLAYERS])\n",
    "\n",
    "name_dict = {}\n",
    "for p in TOP_100_PLAYERS:\n",
    "    name_dict[p.split()[0]] = p\n",
    "    name_dict[p.split()[1]] = p\n",
    "\n",
    "name_dict[\"james\"] = \"lebron james\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_listed = [clean(article) for article in docs]\n",
    "docs = [\" \".join(article) for article in docs_listed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(text):\n",
    "    common = {\"bron\": \"lebron_james\", \"kd\": \"kevin_durant\", \"steph\": \"stephen_curry\", \"russ\": \"russell_westbrook\"}\n",
    "    \n",
    "    original_text = \" \".join([word.lower() for word in text.split() if word.isalpha()])\n",
    "    text = original_text # for modification\n",
    "    \n",
    "    for fn in players_first_names:\n",
    "        wholename = name_dict[fn]\n",
    "        if fn in text and wholename in original_text: # if first name and entire name is somewhere in text\n",
    "            text = text.replace(wholename, \"_\".join(wholename.split())) # join wholename with underscore\n",
    "            text = text.replace(\" \" + fn + \" \", \" \" + \"_\".join(wholename.split()) + \" \") #join the first name\n",
    "    \n",
    "    for ln in players_last_names:\n",
    "        wholename = name_dict[ln]\n",
    "        if ln in text and wholename in original_text: # if first name and entire name is somewhere in text\n",
    "            text = text.replace(wholename, \"_\".join(wholename.split())) # join wholename with underscore\n",
    "            text = text.replace(\" \" + ln + \" \", \" \" + \"_\".join(wholename.split()) + \" \") #join the first name\n",
    "\n",
    "    for team in teams:\n",
    "        if team in text:\n",
    "            text = text.replace(\" \" + team + \" \", \" \" + \"_\".join(team.split()) + \" \")\n",
    "\n",
    "    for city in cities:\n",
    "        if city in text:\n",
    "            text = text.replace(\" \" + city + \" \", \" \" + \"_\".join(city.split()) + \" \")\n",
    "\n",
    "    for name in team_names:\n",
    "        if name in text:\n",
    "            text = text.replace(\" \" + name + \" \", \" \" + \"_\".join(name.split()) + \" \")\n",
    "            \n",
    "    for coach in coaches:\n",
    "        last_name = coach.split()[1]\n",
    "        if coach in text:\n",
    "            text = text.replace(\" \" + coach + \" \", \" \" + \"_\".join(coach.split()) + \" \")\n",
    "            text = text.replace(\" \" + last_name + \" \", \" \" + \"_\".join(coach.split()) + \" \")\n",
    "\n",
    "    \n",
    "    for acr, real in common.items():\n",
    "        text = text.replace(\" \" + acr + \" \", \" \" + real + \" \")\n",
    "    \n",
    "    return text\n",
    "\n",
    "docs = [clean_doc(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3314, 3314)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs), len(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_cleaned = [d for d in docs]\n",
    "docs_cleaned_filename = \"docs_cleaned_saturday2\"\n",
    "with open(docs_cleaned_filename, 'wb') as fp:\n",
    "    pickle.dump(docs_cleaned, fp)\n",
    "    \n",
    "    \n",
    "# with open (docs_cleaned_filename, 'rb') as fp:\n",
    "#     cleaned_docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict_to_file(filename, d):\n",
    "    with open(filename, 'r+') as f:\n",
    "        if len(f.read()) == 0:\n",
    "            f.write(json.dumps(d))\n",
    "        else:\n",
    "            f.write('\\n' + json.dumps(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dicts_filename = \"article_dicts_saturday2\"\n",
    "\n",
    "with open(article_dicts_filename, 'r+') as f:\n",
    "    f.write(json.dumps(all_articles[0]))\n",
    "\n",
    "for ad in all_articles[1:]:\n",
    "    add_dict_to_file(article_dicts_filename, ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_dicts_filename = \"article_dicts_saturday\"\n",
    "# import codecs\n",
    "# all_articles = []\n",
    "# with codecs.open(article_dicts_filename,'rU','utf-8') as f:\n",
    "#     for line in f:\n",
    "#         all_articles.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get the articles now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# param_dict = {\"from\": \"2017-10-17\", \"sources\":\"bleacher-report\",\n",
    "#               \"pageSize\":\"100\", \"apiKey\":API_KEY, \"q\":query} # you can also do \"sortBy=popularity\"\n",
    "# set_of_docs = get_docs_from_params(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_filename = \"all_nba_from_oct17\"\n",
    "# with open(big_filename, 'wb') as fp:\n",
    "#     pickle.dump(set_of_docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (big_filename, 'rb') as fp:\n",
    "#     docs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# players only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bron\n",
      "0/809...50/809...100/809...150/809...200/809...250/809...300/809...350/809...400/809...450/809...500/809...550/809...600/809...650/809...700/809...750/809...800/809...kd\n",
      "0/294...50/294...100/294...150/294...200/294...250/294...steph\n",
      "0/331...50/331...100/331...150/331...200/331...250/331...300/331...harden\n",
      "0/195...50/195...100/195...150/195...russel\n",
      "0/226...50/226...100/226...150/226...200/226...davis\n",
      "0/178...50/178...100/178...150/178..."
     ]
    }
   ],
   "source": [
    "# player_queries = [\"(lebron james) OR lebron\", \"(kevin durant) OR KD\", \"(steph curry) OR (stephen curry)\", \n",
    "#                  \"(james harden) OR (harden)\", \"(russell westbrook) OR (westbrook)\", \"(anthony davis)\"]\n",
    "# player_docs_list = {}\n",
    "# player_names = [\"bron\", \"kd\", \"steph\", \"harden\", \"russel\", \"davis\"]\n",
    "\n",
    "\n",
    "# start_date = \"2017-10-17\" # start of 2017-2018 NBA season\n",
    "# for player, name in zip(player_queries, player_names):\n",
    "#     print(name)\n",
    "#     param_dict = {\"from\": start_date, \"sources\":\"bleacher-report\",\n",
    "#               \"pageSize\":\"100\", \"apiKey\":API_KEY, \"q\":player} # you can also do \"sortBy=popularity\"\n",
    "\n",
    "#     player_docs = get_docs_from_params(param_dict)\n",
    "#     player_docs_list[player] = player_docs\n",
    "\n",
    "# # save to a file\n",
    "# for player, name in zip(player_queries, player_names):\n",
    "#     filename = name + \"_from_oct17_take2\"\n",
    "#     with open(filename, 'wb') as fp:\n",
    "#         pickle.dump(player_docs_list[player], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scrape the comments ... having trouble with this because of the iframe. forget about it for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 394 ms, sys: 36.2 ms, total: 430 ms\n",
      "Wall time: 848 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# comments_by_doc = []\n",
    "# for article in all_articles[:10]:\n",
    "#     comments = []\n",
    "#     url = article['url']\n",
    "#     soup = BeautifulSoup(requests.get(url).text, \"lxml\")\n",
    "#     content = soup.find_all(\"span\", {\"class\":\"_5mdd\"})\n",
    "#     if content is None:\n",
    "#         print(\"no comment with class _5mdd\")\n",
    "#     for comment_span in content:\n",
    "#         comments.append(comment_span.get_text())\n",
    "#     comments_by_doc.append(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
